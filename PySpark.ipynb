{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e4e83d",
   "metadata": {},
   "source": [
    "# PySpark:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b6a720",
   "metadata": {},
   "source": [
    "Pyspark is an interface for Apache Spark in Python is often used for \n",
    "large scale data processing and Machine Learning.\n",
    "Spark with Python is PySpark Library:\n",
    "Pyspark will handle huge amount of data set i,e 128GB in distributed system."
   ]
  },
  {
   "cell_type": "raw",
   "id": "42216276",
   "metadata": {},
   "source": [
    "#PySpark Dataframe\n",
    "Reading The DataSet\n",
    "Checking the DataTypes of the Columns(Schema)\n",
    "Selecting Columns & Indexing\n",
    "Check Describe option similar to Pandas\n",
    "Adding Columns\n",
    "Dropping Columns.\n",
    "Renaming Columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "359758eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\vasan\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\vasan\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vasan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vasan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vasan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vasan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vasan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vasan\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef2db784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d0ea1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Obarai</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vasanth</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pinku</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name  Age\n",
       "0   Obarai   25\n",
       "1  Vasanth   26\n",
       "2    Pinku   27"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('PySpark1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f34c56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f065a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b535be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf763aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pinku.wifi.uwm.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1879844e550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "116f08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('PySpark1.csv') #Read Pyspark %%file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f156a5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ec000e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|    _c0|_c1|\n",
      "+-------+---+\n",
      "|   Name|Age|\n",
      "| Obarai| 25|\n",
      "|Vasanth| 26|\n",
      "|  Pinku| 27|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6657534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.option('header','true').csv('PySpark1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "429b11c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83f67eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Obarai', Age='25'),\n",
       " Row(Name='Vasanth', Age='26'),\n",
       " Row(Name='Pinku', Age='27')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e579d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24556c29",
   "metadata": {},
   "source": [
    "# Build the Pyspark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d308a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "846e20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a17fc442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pinku.wifi.uwm.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1879844e550>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717dc459",
   "metadata": {},
   "source": [
    "# Read the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ab81546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: string, Experience: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the DataSet\n",
    "spark.read.option('header','true').csv('PySpark2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75aebe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.option('header','true').csv('PySpark2.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75da82fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Check the schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d59a26dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option -2 Read the dataset\n",
    "df_pyspark=spark.read.csv('PySpark2.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23388c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| Name|age|Experience|\n",
      "+-----+---+----------+\n",
      "|Pinku| 25|         5|\n",
      "|Tinku| 27|         7|\n",
      "|Sunny| 28|         8|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Check the schema\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04a41588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Check the schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9f27a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f27c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5c2cdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'Experience']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50db5039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Name='Pinku', age=25, Experience=5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8dfedb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Pinku', age=25, Experience=5),\n",
       " Row(Name='Tinku', age=27, Experience=7),\n",
       " Row(Name='Sunny', age=28, Experience=8)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3990c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting columns and perform operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e89ca93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| Name|age|Experience|\n",
      "+-----+---+----------+\n",
      "|Pinku| 25|         5|\n",
      "|Tinku| 27|         7|\n",
      "|Sunny| 28|         8|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0be1eeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| Name|\n",
      "+-----+\n",
      "|Pinku|\n",
      "|Tinku|\n",
      "|Sunny|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1225e293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark.select('Name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82c32f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| Name|Experience|\n",
      "+-----+----------+\n",
      "|Pinku|         5|\n",
      "|Tinku|         7|\n",
      "|Sunny|         8|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name','Experience']).show() #Select two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb549358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Name'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark['Name'] #Pick a columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9dd8d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('age', 'int'), ('Experience', 'int')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes #Select data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20988e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe option similar to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f4c9145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+------------------+\n",
      "|summary| Name|               age|        Experience|\n",
      "+-------+-----+------------------+------------------+\n",
      "|  count|    3|                 3|                 3|\n",
      "|   mean| NULL|26.666666666666668| 6.666666666666667|\n",
      "| stddev| NULL|1.5275252316519468|1.5275252316519468|\n",
      "|    min|Pinku|                25|                 5|\n",
      "|    max|Tinku|                28|                 8|\n",
      "+-------+-----+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73978143",
   "metadata": {},
   "source": [
    "# Adding Columns in Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6679a694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: int, Experience: int, Experience After 2 years: int]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.withColumn('Experience After 2 years',df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec68dc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=df_pyspark.withColumn('Experience After 2 years',df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f931736d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------------------------+\n",
      "| Name|age|Experience|Experience After 2 years|\n",
      "+-----+---+----------+------------------------+\n",
      "|Pinku| 25|         5|                       7|\n",
      "|Tinku| 27|         7|                       9|\n",
      "|Sunny| 28|         8|                      10|\n",
      "+-----+---+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcceb71",
   "metadata": {},
   "source": [
    "# Dropping Columns in Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b70acfe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: int, Experience: int]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.drop('Experience After 2 years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "125f3965",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=df_pyspark.drop('Experience After 2 years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e521dff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| Name|age|Experience|\n",
      "+-----+---+----------+\n",
      "|Pinku| 25|         5|\n",
      "|Tinku| 27|         7|\n",
      "|Sunny| 28|         8|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5915c20",
   "metadata": {},
   "source": [
    "# Rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b85a4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|New Name|age|Experience|\n",
      "+--------+---+----------+\n",
      "|   Pinku| 25|         5|\n",
      "|   Tinku| 27|         7|\n",
      "|   Sunny| 28|         8|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumnRenamed('Name','New Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc82ef2",
   "metadata": {},
   "source": [
    "# PySpark Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ff4e6f",
   "metadata": {},
   "source": [
    "#Dropping Columns\n",
    "\n",
    "#Dropping Rows\n",
    "\n",
    "#Various Parameter In Dropping Functionalities\n",
    "\n",
    "#Handling Missing Values by Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd9a141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read The Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5fd5fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('PySpark3.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea9a55a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+------+\n",
      "|     Name| age|Experiene|Salary|\n",
      "+---------+----+---------+------+\n",
      "|   Krish |  31|       10| 30000|\n",
      "|Sudhanshu|  30|        8| 25000|\n",
      "|    Sunny|  29|        4| 20000|\n",
      "|     Paul|  24|        3| 20000|\n",
      "|   Harsha|  21|        1| 15000|\n",
      "|  Shubham|  23|        2| 18000|\n",
      "|   Mahesh|NULL|     NULL| 40000|\n",
      "|     NULL|  34|       10| 38000|\n",
      "|     NULL|  36|     NULL|  NULL|\n",
      "+---------+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73793a82",
   "metadata": {},
   "source": [
    "# Drop the Missing values columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3ba47f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------+\n",
      "| age|Experiene|Salary|\n",
      "+----+---------+------+\n",
      "|  31|       10| 30000|\n",
      "|  30|        8| 25000|\n",
      "|  29|        4| 20000|\n",
      "|  24|        3| 20000|\n",
      "|  21|        1| 15000|\n",
      "|  23|        2| 18000|\n",
      "|NULL|     NULL| 40000|\n",
      "|  34|       10| 38000|\n",
      "|  36|     NULL|  NULL|\n",
      "+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a52756a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+------+\n",
      "|     Name| age|Experiene|Salary|\n",
      "+---------+----+---------+------+\n",
      "|   Krish |  31|       10| 30000|\n",
      "|Sudhanshu|  30|        8| 25000|\n",
      "|    Sunny|  29|        4| 20000|\n",
      "|     Paul|  24|        3| 20000|\n",
      "|   Harsha|  21|        1| 15000|\n",
      "|  Shubham|  23|        2| 18000|\n",
      "|   Mahesh|NULL|     NULL| 40000|\n",
      "|     NULL|  34|       10| 38000|\n",
      "|     NULL|  36|     NULL|  NULL|\n",
      "+---------+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "22341216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1aa703b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------+------+\n",
      "|     Name|age|Experiene|Salary|\n",
      "+---------+---+---------+------+\n",
      "|   Krish | 31|       10| 30000|\n",
      "|Sudhanshu| 30|        8| 25000|\n",
      "|    Sunny| 29|        4| 20000|\n",
      "|     Paul| 24|        3| 20000|\n",
      "|   Harsha| 21|        1| 15000|\n",
      "|  Shubham| 23|        2| 18000|\n",
      "+---------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show() #.na .drop drop all null value rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49e5b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## any=How"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "692b1b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+------+\n",
      "|     Name| age|Experiene|Salary|\n",
      "+---------+----+---------+------+\n",
      "|   Krish |  31|       10| 30000|\n",
      "|Sudhanshu|  30|        8| 25000|\n",
      "|    Sunny|  29|        4| 20000|\n",
      "|     Paul|  24|        3| 20000|\n",
      "|   Harsha|  21|        1| 15000|\n",
      "|  Shubham|  23|        2| 18000|\n",
      "|   Mahesh|NULL|     NULL| 40000|\n",
      "|     NULL|  34|       10| 38000|\n",
      "|     NULL|  36|     NULL|  NULL|\n",
      "+---------+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"all\").show() #Remove all rows having all Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e1fa6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d93b9aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+------+\n",
      "|     Name| age|Experiene|Salary|\n",
      "+---------+----+---------+------+\n",
      "|   Krish |  31|       10| 30000|\n",
      "|Sudhanshu|  30|        8| 25000|\n",
      "|    Sunny|  29|        4| 20000|\n",
      "|     Paul|  24|        3| 20000|\n",
      "|   Harsha|  21|        1| 15000|\n",
      "|  Shubham|  23|        2| 18000|\n",
      "|   Mahesh|NULL|     NULL| 40000|\n",
      "|     NULL|  34|       10| 38000|\n",
      "+---------+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"any\",thresh=2).show() #Atleast two Non Null Values present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7b09f151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+------+\n",
      "|     Name| age|Experiene|Salary|\n",
      "+---------+----+---------+------+\n",
      "|   Krish |  31|       10| 30000|\n",
      "|Sudhanshu|  30|        8| 25000|\n",
      "|    Sunny|  29|        4| 20000|\n",
      "|     Paul|  24|        3| 20000|\n",
      "|   Harsha|  21|        1| 15000|\n",
      "|  Shubham|  23|        2| 18000|\n",
      "|   Mahesh|NULL|     NULL| 40000|\n",
      "|     NULL|  34|       10| 38000|\n",
      "|     NULL|  36|     NULL|  NULL|\n",
      "+---------+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"any\",thresh=1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d2504e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------+------+\n",
      "|     Name|age|Experiene|Salary|\n",
      "+---------+---+---------+------+\n",
      "|   Krish | 31|       10| 30000|\n",
      "|Sudhanshu| 30|        8| 25000|\n",
      "|    Sunny| 29|        4| 20000|\n",
      "|     Paul| 24|        3| 20000|\n",
      "|   Harsha| 21|        1| 15000|\n",
      "|  Shubham| 23|        2| 18000|\n",
      "|     NULL| 34|       10| 38000|\n",
      "+---------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"any\",thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75c66c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##SubSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "76914da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------+------+\n",
      "|     Name|age|Experiene|Salary|\n",
      "+---------+---+---------+------+\n",
      "|   Krish | 31|       10| 30000|\n",
      "|Sudhanshu| 30|        8| 25000|\n",
      "|    Sunny| 29|        4| 20000|\n",
      "|     Paul| 24|        3| 20000|\n",
      "|   Harsha| 21|        1| 15000|\n",
      "|  Shubham| 23|        2| 18000|\n",
      "|     NULL| 34|       10| 38000|\n",
      "|     NULL| 36|     NULL|  NULL|\n",
      "+---------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"any\",subset=['Age']).show() #Row containing null values deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f0fcf924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------+------+\n",
      "|     Name|age|Experiene|Salary|\n",
      "+---------+---+---------+------+\n",
      "|   Krish | 31|       10| 30000|\n",
      "|Sudhanshu| 30|        8| 25000|\n",
      "|    Sunny| 29|        4| 20000|\n",
      "|     Paul| 24|        3| 20000|\n",
      "|   Harsha| 21|        1| 15000|\n",
      "|  Shubham| 23|        2| 18000|\n",
      "|     NULL| 34|       10| 38000|\n",
      "+---------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"any\",subset=['Experiene']).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dc76bb",
   "metadata": {},
   "source": [
    "# Fill The Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a392050f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+---------+------+\n",
      "|          Name| age|Experiene|Salary|\n",
      "+--------------+----+---------+------+\n",
      "|        Krish |  31|       10| 30000|\n",
      "|     Sudhanshu|  30|        8| 25000|\n",
      "|         Sunny|  29|        4| 20000|\n",
      "|          Paul|  24|        3| 20000|\n",
      "|        Harsha|  21|        1| 15000|\n",
      "|       Shubham|  23|        2| 18000|\n",
      "|        Mahesh|NULL|     NULL| 40000|\n",
      "|Missing Vlaues|  34|       10| 38000|\n",
      "|Missing Vlaues|  36|     NULL|  NULL|\n",
      "+--------------+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing Vlaues').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "632577c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+------+\n",
      "|     Name| age|Experiene|Salary|\n",
      "+---------+----+---------+------+\n",
      "|   Krish |  31|       10| 30000|\n",
      "|Sudhanshu|  30|        8| 25000|\n",
      "|    Sunny|  29|        4| 20000|\n",
      "|     Paul|  24|        3| 20000|\n",
      "|   Harsha|  21|        1| 15000|\n",
      "|  Shubham|  23|        2| 18000|\n",
      "|   Mahesh|NULL|     NULL| 40000|\n",
      "|     NULL|  34|       10| 38000|\n",
      "|     NULL|  36|     NULL|  NULL|\n",
      "+---------+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing Vlaues',['Experiene','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6bae8b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+------+\n",
      "|     Name| age|Experiene|Salary|\n",
      "+---------+----+---------+------+\n",
      "|   Krish |  31|       10| 30000|\n",
      "|Sudhanshu|  30|        8| 25000|\n",
      "|    Sunny|  29|        4| 20000|\n",
      "|     Paul|  24|        3| 20000|\n",
      "|   Harsha|  21|        1| 15000|\n",
      "|  Shubham|  23|        2| 18000|\n",
      "|   Mahesh|NULL|     NULL| 40000|\n",
      "|     NULL|  34|       10| 38000|\n",
      "|     NULL|  36|     NULL|  NULL|\n",
      "+---------+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0cd09a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace Experience columns with mean missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3f346f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['age', 'Experiene', 'Salary'], \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experiene', 'Salary']]\n",
    "    ).setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "01c7bdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+------+-----------+-----------------+--------------+\n",
      "|     Name| age|Experiene|Salary|age_imputed|Experiene_imputed|Salary_imputed|\n",
      "+---------+----+---------+------+-----------+-----------------+--------------+\n",
      "|   Krish |  31|       10| 30000|         31|               10|         30000|\n",
      "|Sudhanshu|  30|        8| 25000|         30|                8|         25000|\n",
      "|    Sunny|  29|        4| 20000|         29|                4|         20000|\n",
      "|     Paul|  24|        3| 20000|         24|                3|         20000|\n",
      "|   Harsha|  21|        1| 15000|         21|                1|         15000|\n",
      "|  Shubham|  23|        2| 18000|         23|                2|         18000|\n",
      "|   Mahesh|NULL|     NULL| 40000|         29|                4|         40000|\n",
      "|     NULL|  34|       10| 38000|         34|               10|         38000|\n",
      "|     NULL|  36|     NULL|  NULL|         36|                4|         20000|\n",
      "+---------+----+---------+------+-----------+-----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "49463cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['age', 'Experiene', 'Salary'], \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experiene', 'Salary']]\n",
    "    ).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fe834fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+------+-----------+-----------------+--------------+\n",
      "|     Name| age|Experiene|Salary|age_imputed|Experiene_imputed|Salary_imputed|\n",
      "+---------+----+---------+------+-----------+-----------------+--------------+\n",
      "|   Krish |  31|       10| 30000|         31|               10|         30000|\n",
      "|Sudhanshu|  30|        8| 25000|         30|                8|         25000|\n",
      "|    Sunny|  29|        4| 20000|         29|                4|         20000|\n",
      "|     Paul|  24|        3| 20000|         24|                3|         20000|\n",
      "|   Harsha|  21|        1| 15000|         21|                1|         15000|\n",
      "|  Shubham|  23|        2| 18000|         23|                2|         18000|\n",
      "|   Mahesh|NULL|     NULL| 40000|         29|                4|         40000|\n",
      "|     NULL|  34|       10| 38000|         34|               10|         38000|\n",
      "|     NULL|  36|     NULL|  NULL|         36|                4|         20000|\n",
      "+---------+----+---------+------+-----------+-----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436d6fd0",
   "metadata": {},
   "source": [
    "# PySpark Dataframes\n",
    "Filter Operation\n",
    "\n",
    "&,|,==\n",
    "\n",
    "~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7504cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8326e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4e3c7a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+------+\n",
      "|     Name| age|Experiene|Salary|\n",
      "+---------+----+---------+------+\n",
      "|   Krish |  31|       10| 30000|\n",
      "|Sudhanshu|  30|        8| 25000|\n",
      "|    Sunny|  29|        4| 20000|\n",
      "|     Paul|  24|        3| 20000|\n",
      "|   Harsha|  21|        1| 15000|\n",
      "|  Shubham|  23|        2| 18000|\n",
      "|   Mahesh|NULL|     NULL| 40000|\n",
      "|     NULL|  34|       10| 38000|\n",
      "|     NULL|  36|     NULL|  NULL|\n",
      "+---------+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=spark.read.csv('PySpark3.csv',header=True,inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570e405f",
   "metadata": {},
   "source": [
    "# Filter Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b6a5d2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+------+\n",
      "|   Name|age|Experiene|Salary|\n",
      "+-------+---+---------+------+\n",
      "|  Sunny| 29|        4| 20000|\n",
      "|   Paul| 24|        3| 20000|\n",
      "| Harsha| 21|        1| 15000|\n",
      "|Shubham| 23|        2| 18000|\n",
      "+-------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Salary of the people less than or equal to 20000\n",
    "df_pyspark.filter(\"Salary<=20000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e3bf1c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|age|\n",
      "+-------+---+\n",
      "|  Sunny| 29|\n",
      "|   Paul| 24|\n",
      "| Harsha| 21|\n",
      "|Shubham| 23|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(\"Salary<=20000\").select(['Name','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "75f1a9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+------+\n",
      "|   Name|age|Experiene|Salary|\n",
      "+-------+---+---------+------+\n",
      "|  Sunny| 29|        4| 20000|\n",
      "|   Paul| 24|        3| 20000|\n",
      "| Harsha| 21|        1| 15000|\n",
      "|Shubham| 23|        2| 18000|\n",
      "+-------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(df_pyspark['Salary']<=20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a64b8fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+------+\n",
      "|     Name| age|Experiene|Salary|\n",
      "+---------+----+---------+------+\n",
      "|   Krish |  31|       10| 30000|\n",
      "|Sudhanshu|  30|        8| 25000|\n",
      "|    Sunny|  29|        4| 20000|\n",
      "|     Paul|  24|        3| 20000|\n",
      "|   Harsha|  21|        1| 15000|\n",
      "|  Shubham|  23|        2| 18000|\n",
      "|   Mahesh|NULL|     NULL| 40000|\n",
      "|     NULL|  34|       10| 38000|\n",
      "+---------+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['Salary']<=20000) | \n",
    "                  (df_pyspark['Salary']>=15000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3cb577d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+------+\n",
      "|     Name| age|Experiene|Salary|\n",
      "+---------+----+---------+------+\n",
      "|   Krish |  31|       10| 30000|\n",
      "|Sudhanshu|  30|        8| 25000|\n",
      "|   Mahesh|NULL|     NULL| 40000|\n",
      "|     NULL|  34|       10| 38000|\n",
      "+---------+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(~(df_pyspark['Salary']<=20000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3e70a0",
   "metadata": {},
   "source": [
    "# Pyspark GroupBy & Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ac96fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "72677aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Agg').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "433cb717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pinku.wifi.uwm.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1879844e550>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0038c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('PySpark4.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ab630b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|Salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "671c4c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5feba6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(Salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      35000|\n",
      "|    Sunny|      12000|\n",
      "|    Krish|      19000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Groupby\n",
    "### Grouped to find the maximum salary\n",
    "df_pyspark.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "be0a3346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|     Name|       avg(Salary)|\n",
      "+---------+------------------+\n",
      "|Sudhanshu|11666.666666666666|\n",
      "|    Sunny|            6000.0|\n",
      "|    Krish| 6333.333333333333|\n",
      "|   Mahesh|            3500.0|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d24f39fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|sum(Salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Groupby Departmernts  which gives maximum salary\n",
    "df_pyspark.groupBy('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0da960f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|avg(Salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     3750.0|\n",
      "|Data Science|    10750.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Departments').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "894149d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Departments').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0965e8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      73000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8af3f4e",
   "metadata": {},
   "source": [
    "# Example Of PySpark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "258681d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Missing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3e5bb7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read The dataset\n",
    "training = spark.read.csv('test1.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c5df52f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ae0f248d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3f5ed8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bf634090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler=VectorAssembler(inputCols=[\"age\",\"Experience\"],outputCol=\"Independent Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f9c8a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=featureassembler.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bd9cccbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+--------------------+\n",
      "|     Name|age|Experience|Salary|Independent Features|\n",
      "+---------+---+----------+------+--------------------+\n",
      "|    Krish| 31|        10| 30000|         [31.0,10.0]|\n",
      "|Sudhanshu| 30|         8| 25000|          [30.0,8.0]|\n",
      "|    Sunny| 29|         4| 20000|          [29.0,4.0]|\n",
      "|     Paul| 24|         3| 20000|          [24.0,3.0]|\n",
      "|   Harsha| 21|         1| 15000|          [21.0,1.0]|\n",
      "|  Shubham| 23|         2| 18000|          [23.0,2.0]|\n",
      "+---------+---+----------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "18184e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'Experience', 'Salary', 'Independent Features']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bfc67b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data=output.select(\"Independent Features\",\"Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d5c54e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|Independent Features|Salary|\n",
      "+--------------------+------+\n",
      "|         [31.0,10.0]| 30000|\n",
      "|          [30.0,8.0]| 25000|\n",
      "|          [29.0,4.0]| 20000|\n",
      "|          [24.0,3.0]| 20000|\n",
      "|          [21.0,1.0]| 15000|\n",
      "|          [23.0,2.0]| 18000|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fa5cb47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "##train test split\n",
    "train_data,test_data=finalized_data.randomSplit([0.75,0.25])\n",
    "regressor=LinearRegression(featuresCol='Independent Features', labelCol='Salary')\n",
    "regressor=regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7b1d5eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0, 1589.7436])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Coefficients\n",
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "16ba7398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13358.974358973992"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Intercepts\n",
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "89b62ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prediction\n",
    "pred_results=regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f9cb6110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+\n",
      "|Independent Features|Salary|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|          [23.0,2.0]| 18000|16538.461538461514|\n",
      "|          [24.0,3.0]| 20000|18128.205128205107|\n",
      "+--------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3212ac3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1666.6666666666897, 2819855.358316973)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.meanAbsoluteError,pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c7cc9",
   "metadata": {},
   "source": [
    "# Linear Regression with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6c0fcb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# File location and type\n",
    "file_location = \"/Users/vasan/tipspyspark.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df =spark.read.csv(file_location,header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "db86eed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- total_bill: double (nullable = true)\n",
      " |-- tip: double (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- smoker: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- size: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "fe04eafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "191a110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Handling Categorical Features\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c574860e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c6ad2249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-----------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|sex_indexed|\n",
      "+----------+----+------+------+---+------+----+-----------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|        1.0|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|        0.0|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|        0.0|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|        0.0|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|        1.0|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|        0.0|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|        0.0|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|        0.0|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|        0.0|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|        0.0|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|        0.0|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|        1.0|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|        0.0|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|        0.0|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|        1.0|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|        0.0|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|        1.0|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|        0.0|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|        1.0|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|        0.0|\n",
      "+----------+----+------+------+---+------+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer=StringIndexer(inputCol=\"sex\",outputCol=\"sex_indexed\")\n",
    "df_r=indexer.fit(df).transform(df)\n",
    "df_r.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5c317a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+----------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|sex_indexed|smoker_indexed|day_indexed|time_index|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+----------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|        1.0|           0.0|        1.0|       0.0|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|       0.0|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|       0.0|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|        1.0|           0.0|        1.0|       0.0|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|       0.0|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|       0.0|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|        1.0|           0.0|        1.0|       0.0|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|       0.0|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|        1.0|           0.0|        1.0|       0.0|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|        1.0|           0.0|        1.0|       0.0|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|       0.0|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|        1.0|           0.0|        1.0|       0.0|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|        0.0|           0.0|        0.0|       0.0|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer=StringIndexer(inputCols=[\"smoker\",\"day\",\"time\"],outputCols=[\"smoker_indexed\",\"day_indexed\",\n",
    "                                                                  \"time_index\"])\n",
    "df_r=indexer.fit(df_r).transform(df_r)\n",
    "df_r.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4d6e990c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['total_bill',\n",
       " 'tip',\n",
       " 'sex',\n",
       " 'smoker',\n",
       " 'day',\n",
       " 'time',\n",
       " 'size',\n",
       " 'sex_indexed',\n",
       " 'smoker_indexed',\n",
       " 'day_indexed',\n",
       " 'time_index']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_r.columns\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "cfcc2d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler=VectorAssembler(inputCols=['tip','size','sex_indexed','smoker_indexed','day_indexed',\n",
    "                          'time_index'],outputCol=\"Independent Features\")\n",
    "output=featureassembler.transform(df_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a14d6135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|Independent Features|\n",
      "+--------------------+\n",
      "|[1.01,2.0,1.0,0.0...|\n",
      "|[1.66,3.0,0.0,0.0...|\n",
      "|[3.5,3.0,0.0,0.0,...|\n",
      "|[3.31,2.0,0.0,0.0...|\n",
      "|[3.61,4.0,1.0,0.0...|\n",
      "|[4.71,4.0,0.0,0.0...|\n",
      "|[2.0,2.0,0.0,0.0,...|\n",
      "|[3.12,4.0,0.0,0.0...|\n",
      "|[1.96,2.0,0.0,0.0...|\n",
      "|[3.23,2.0,0.0,0.0...|\n",
      "|[1.71,2.0,0.0,0.0...|\n",
      "|[5.0,4.0,1.0,0.0,...|\n",
      "|[1.57,2.0,0.0,0.0...|\n",
      "|[3.0,4.0,0.0,0.0,...|\n",
      "|[3.02,2.0,1.0,0.0...|\n",
      "|[3.92,2.0,0.0,0.0...|\n",
      "|[1.67,3.0,1.0,0.0...|\n",
      "|[3.71,3.0,0.0,0.0...|\n",
      "|[3.5,3.0,1.0,0.0,...|\n",
      "|(6,[0,1],[3.35,3.0])|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.select('Independent Features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6c04c9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+----------+--------------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|sex_indexed|smoker_indexed|day_indexed|time_index|Independent Features|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+----------+--------------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|        1.0|           0.0|        1.0|       0.0|[1.01,2.0,1.0,0.0...|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|       0.0|[1.66,3.0,0.0,0.0...|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|       0.0|[3.5,3.0,0.0,0.0,...|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|[3.31,2.0,0.0,0.0...|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|        1.0|           0.0|        1.0|       0.0|[3.61,4.0,1.0,0.0...|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|       0.0|[4.71,4.0,0.0,0.0...|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|[2.0,2.0,0.0,0.0,...|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|       0.0|[3.12,4.0,0.0,0.0...|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|[1.96,2.0,0.0,0.0...|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|[3.23,2.0,0.0,0.0...|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|[1.71,2.0,0.0,0.0...|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|        1.0|           0.0|        1.0|       0.0|[5.0,4.0,1.0,0.0,...|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|[1.57,2.0,0.0,0.0...|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|       0.0|[3.0,4.0,0.0,0.0,...|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|        1.0|           0.0|        1.0|       0.0|[3.02,2.0,1.0,0.0...|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|[3.92,2.0,0.0,0.0...|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|        1.0|           0.0|        1.0|       0.0|[1.67,3.0,1.0,0.0...|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|       0.0|[3.71,3.0,0.0,0.0...|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|        1.0|           0.0|        1.0|       0.0|[3.5,3.0,1.0,0.0,...|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|        0.0|           0.0|        0.0|       0.0|(6,[0,1],[3.35,3.0])|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "480df4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data=output.select(\"Independent Features\",\"total_bill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b0e1f768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|Independent Features|Salary|\n",
      "+--------------------+------+\n",
      "|         [31.0,10.0]| 30000|\n",
      "|          [30.0,8.0]| 25000|\n",
      "|          [29.0,4.0]| 20000|\n",
      "|          [24.0,3.0]| 20000|\n",
      "|          [21.0,1.0]| 15000|\n",
      "|          [23.0,2.0]| 18000|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "bf7da3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "##train test split\n",
    "train_data,test_data=finalized_data.randomSplit([0.75,0.25])\n",
    "regressor=LinearRegression(featuresCol='Independent Features', labelCol='total_bill')\n",
    "regressor=regressor.fit(train_data)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "14a06bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([2.6152, 3.6633, -0.6616, 1.7421, -0.0792, -1.486])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f774bdbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4529241393132795"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "8eaebe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predictions\n",
    "pred_results=regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e04c35ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------------+\n",
      "|Independent Features|total_bill|        prediction|\n",
      "+--------------------+----------+------------------+\n",
      "|(6,[0,1],[1.25,2.0])|     10.07|13.048557535644939|\n",
      "|(6,[0,1],[1.75,2.0])|     17.82| 14.35617147427661|\n",
      "| (6,[0,1],[3.6,3.0])|     24.06|22.857642322090037|\n",
      "|(6,[0,1],[7.58,4.0])|     39.42|36.929548548474386|\n",
      "| (6,[0,1],[9.0,4.0])|     48.33| 40.64317213418833|\n",
      "|[1.0,1.0,1.0,1.0,...|      3.07| 9.811990486537056|\n",
      "|[1.5,2.0,0.0,1.0,...|     15.69|15.365245482929115|\n",
      "|[1.56,2.0,0.0,0.0...|      9.94|13.780035589239283|\n",
      "|[1.57,2.0,0.0,0.0...|     15.42|13.806187868011918|\n",
      "|[1.61,2.0,1.0,1.0...|     10.59|15.070578766543935|\n",
      "|[1.66,3.0,0.0,0.0...|     10.34|17.704857651841856|\n",
      "|[1.71,2.0,0.0,0.0...|     10.27|14.172319770828786|\n",
      "|[1.73,2.0,0.0,0.0...|      9.78| 12.65936457912624|\n",
      "|[1.83,1.0,1.0,0.0...|     10.07| 8.596003720734892|\n",
      "|[1.92,1.0,0.0,1.0...|      8.58|11.155839578898373|\n",
      "|[2.0,2.0,0.0,0.0,...|      8.77|14.930735855235152|\n",
      "|[2.0,2.0,0.0,1.0,...|     13.81| 16.75210200991808|\n",
      "|[2.0,2.0,0.0,1.0,...|     22.67| 16.75210200991808|\n",
      "|[2.0,2.0,0.0,1.0,...|     10.34|15.107599672312976|\n",
      "|[2.0,2.0,1.0,0.0,...|     14.15|12.703891734745902|\n",
      "+--------------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Final comparison\n",
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c2ca2e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5944799922474994, 5.149157217970128, 49.322048740625135)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PErformance Metrics\n",
    "pred_results.r2,pred_results.meanAbsoluteError,pred_results.meanSquaredError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
